---
title: "YAML Configuration Basics"
description: "Understand the structure and components of agent configuration files"
---

## Configuration Structure

Every agent system in Atthene Agents Studio is defined by a YAML configuration file. This file describes:

- What agents exist
- How they behave
- What tools they can use
- How they connect to knowledge bases
- How they route between each other

---

## Minimal Configuration

Here's the simplest possible agent configuration:

```yaml
name: "My Simple Agent"
description: "A basic conversational agent"
save_messages: true
persistent_state: true

agents:
  - name: "assistant"
    agent_type: "llm_agent"
    description: "Main assistant"
    system_prompt: "You are a helpful AI assistant."
    llm_config:
      temperature: 0.7
      max_tokens: 1000

edges:
  - from: "__start__"
    to: "assistant"
  - from: "assistant"
    to: "__end__"

entry_point: "assistant"
```

Let's break down each section.

---

## Top-Level Configuration

### System Metadata

```yaml
name: "My Agent System"
description: "What this system does"
save_messages: true
persistent_state: true
```

<ParamField path="name" type="string" required>
  Display name for your agent system
</ParamField>

<ParamField path="description" type="string">
  Human-readable description of what this system does
</ParamField>

<ParamField path="save_messages" type="boolean" default="true">
  Whether to persist conversation messages in the database
</ParamField>

<ParamField path="persistent_state" type="boolean" default="true">
  Whether to maintain state across conversation sessions
</ParamField>

---

## Agent Definitions

The `agents` array contains one or more agent configurations:

```yaml
agents:
  - name: "research_agent"
    agent_type: "llm_agent"
    description: "Researches topics using available tools"
    system_prompt: "You are a research specialist..."
    llm_config:
      temperature: 0.7
      max_tokens: 2000
    tools: ["web_search", "calculator"]
    knowledge_bases: ["company_docs"]
    streaming_config:
      show_output_to_user: true
```

### Core Agent Parameters

<ParamField path="name" type="string" required>
  Unique identifier for this agent within the system
</ParamField>

<ParamField path="agent_type" type="string" required>
  Type of agent implementation. Options:
  - `llm_agent`: Standard LLM-based agent
  - `tool_executor`: Specialized tool execution agent
  - More types in advanced configurations
</ParamField>

<ParamField path="description" type="string">
  What this specific agent is responsible for
</ParamField>

<ParamField path="system_prompt" type="string" required>
  The system prompt that defines the agent's behavior and personality
</ParamField>

### LLM Configuration

```yaml
llm_config:
  temperature: 0.7
  max_tokens: 2000
  model: "gpt-4"  # Optional, uses system default if not specified
```

<ParamField path="temperature" type="number" default="0.7">
  Controls randomness in responses (0.0 = deterministic, 1.0 = creative)
</ParamField>

<ParamField path="max_tokens" type="number" default="1000">
  Maximum tokens the agent can generate in a single response
</ParamField>

<ParamField path="model" type="string">
  Specific LLM model to use (optional, falls back to system default)
</ParamField>

### Tool Integration

```yaml
tools:
  - "web_search"
  - "calculator"
  - "custom_tool_name"
```

List of tool names this agent can access. Tools must be:
- Available in the Tools section of the platform
- Properly configured with required parameters
- Granted to the agent via this list

<Tip>
Leave the `tools` array empty (`[]`) if your agent doesn't need external tools.
</Tip>

### Knowledge Base Connection

```yaml
knowledge_bases:
  - "company_policies"
  - "product_documentation"
```

List of knowledge base names to make available for retrieval. The agent will:
- Automatically search these sources when relevant
- Include retrieved context in its reasoning
- Cite sources when configured to do so

<Warning>
Knowledge base names must match exactly with the names you set when uploading data sources.
</Warning>

### Streaming Configuration

```yaml
streaming_config:
  show_output_to_user: true
```

<ParamField path="show_output_to_user" type="boolean" default="true">
  Whether to stream agent output in real-time to end users
</ParamField>

### Memory Configuration

```yaml
memory_config:
  enabled: true
  max_messages: 10
```

<ParamField path="enabled" type="boolean" default="true">
  Enable conversation memory for this agent
</ParamField>

<ParamField path="max_messages" type="number">
  Maximum number of previous messages to include in context
</ParamField>

---

## Workflow Definition

### Edges (Routing)

Edges define how conversations flow between agents:

```yaml
edges:
  - from: "__start__"
    to: "triage_agent"
  
  - from: "triage_agent"
    to: "research_agent"
    condition: "research_needed"
  
  - from: "triage_agent"
    to: "support_agent"
    condition: "support_needed"
  
  - from: "research_agent"
    to: "__end__"
  
  - from: "support_agent"
    to: "__end__"
```

<ParamField path="from" type="string" required>
  Source agent name, or `__start__` for entry point
</ParamField>

<ParamField path="to" type="string" required>
  Destination agent name, or `__end__` to complete
</ParamField>

<ParamField path="condition" type="string">
  Optional condition for conditional routing (advanced)
</ParamField>

### Special Nodes

- **`__start__`**: Entry point for all conversations
- **`__end__`**: Exit point that completes the workflow

Every workflow must have:
- At least one edge from `__start__`
- At least one path to `__end__`

### Entry Point

```yaml
entry_point: "triage_agent"
```

The agent name that receives the initial user message. Must match one of the agents defined in the `agents` array.

---

## Reusable Prompts

Instead of hardcoding prompts in your YAML, you can reference saved prompts from the Prompt Library:

```yaml
agents:
  - name: "assistant"
    agent_type: "llm_agent"
    system_prompt: "{{ prompt('helpful-assistant', 'v2') }}"
```

**Syntax**: `{{ prompt('prompt-name', 'version') }}`

### Benefits

- **Version Control**: Update prompts without changing agent configs
- **Reusability**: Use the same prompt across multiple agents
- **A/B Testing**: Switch between prompt versions easily
- **Collaboration**: Share prompts across your team

<Steps>
  <Step title="Create the Prompt">
    Go to **Prompt Library** and create a new prompt
  </Step>
  <Step title="Version It">
    Give it a version identifier (e.g., 'v1', 'v2', 'production')
  </Step>
  <Step title="Reference It">
    Use the `{{ prompt() }}` syntax in your YAML
  </Step>
</Steps>

---

## Complete Example: Multi-Agent System

Here's a more complex configuration with multiple agents:

```yaml
name: "Customer Support System"
description: "Multi-agent system for customer inquiries"
save_messages: true
persistent_state: true

agents:
  # First agent: Triages incoming requests
  - name: "triage"
    agent_type: "llm_agent"
    description: "Determines the type of customer inquiry"
    system_prompt: "{{ prompt('triage-agent', 'v1') }}"
    llm_config:
      temperature: 0.3
      max_tokens: 500
    tools: []
    knowledge_bases: []
    streaming_config:
      show_output_to_user: false

  # Second agent: Handles technical questions
  - name: "technical_support"
    agent_type: "llm_agent"
    description: "Answers technical questions using documentation"
    system_prompt: "{{ prompt('technical-support', 'v1') }}"
    llm_config:
      temperature: 0.5
      max_tokens: 2000
    tools: ["search_docs", "check_status"]
    knowledge_bases: ["technical_docs", "api_reference"]
    streaming_config:
      show_output_to_user: true

  # Third agent: Handles billing questions
  - name: "billing_support"
    agent_type: "llm_agent"
    description: "Handles billing and account questions"
    system_prompt: "{{ prompt('billing-support', 'v1') }}"
    llm_config:
      temperature: 0.4
      max_tokens: 1500
    tools: ["lookup_account", "check_invoice"]
    knowledge_bases: ["billing_policies"]
    streaming_config:
      show_output_to_user: true

edges:
  # Entry point
  - from: "__start__"
    to: "triage"
  
  # Conditional routing based on triage
  - from: "triage"
    to: "technical_support"
    condition: "technical_issue"
  
  - from: "triage"
    to: "billing_support"
    condition: "billing_issue"
  
  # Both specialized agents return to end
  - from: "technical_support"
    to: "__end__"
  
  - from: "billing_support"
    to: "__end__"

entry_point: "triage"
```

---

## Configuration Validation

Before saving or running your configuration:

1. Click **"Validate YAML"** in the Playground
2. The system checks for:
   - Valid YAML syntax
   - Required fields present
   - Referenced agents exist in edges
   - Knowledge bases and tools are valid
   - Entry point matches an agent name

<Warning>
Always validate before saving to catch errors early. Invalid configurations can't be deployed.
</Warning>

---

## Best Practices

<AccordionGroup>
  <Accordion title="Use Descriptive Names">
    ```yaml
    # Good
    name: "technical_support_tier_1"
    
    # Avoid
    name: "agent1"
    ```
    
    Clear names make debugging and collaboration easier.
  </Accordion>
  
  <Accordion title="Start with Low Temperature">
    ```yaml
    llm_config:
      temperature: 0.3  # Start conservative
    ```
    
    Lower temperatures give more consistent results. Increase only if you need creativity.
  </Accordion>
  
  <Accordion title="Limit Max Tokens">
    ```yaml
    llm_config:
      max_tokens: 1500  # Prevents runaway costs
    ```
    
    Set appropriate limits to control costs and response times.
  </Accordion>
  
  <Accordion title="Use Prompt Library for Production">
    ```yaml
    system_prompt: "{{ prompt('my-prompt', 'v1') }}"
    ```
    
    Easier to update and maintain than inline prompts.
  </Accordion>
  
  <Accordion title="Test Incrementally">
    Start with a single agent, test it thoroughly, then add complexity. Don't build a 5-agent system from scratch.
  </Accordion>
</AccordionGroup>

---

## Common Patterns

### Single Agent (Simple Chatbot)

```yaml
agents:
  - name: "chatbot"
    agent_type: "llm_agent"
    system_prompt: "You are a helpful assistant."
    llm_config:
      temperature: 0.7
      max_tokens: 1000

edges:
  - from: "__start__"
    to: "chatbot"
  - from: "chatbot"
    to: "__end__"

entry_point: "chatbot"
```

### Sequential Pipeline

```yaml
edges:
  - from: "__start__"
    to: "agent1"
  - from: "agent1"
    to: "agent2"
  - from: "agent2"
    to: "agent3"
  - from: "agent3"
    to: "__end__"
```

### Conditional Branching

```yaml
edges:
  - from: "__start__"
    to: "router"
  - from: "router"
    to: "option_a"
    condition: "case_a"
  - from: "router"
    to: "option_b"
    condition: "case_b"
  - from: "option_a"
    to: "__end__"
  - from: "option_b"
    to: "__end__"
```

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Connect Your Agent" icon="plug" href="/guides/connecting-agents">
    Deploy via REST API, WebSocket, or embedded chatbot
  </Card>
  <Card title="Multi-Agent Workflows" icon="diagram-project" href="/guides/multi-agent-workflows">
    Build complex systems with agent coordination
  </Card>
</CardGroup>

---

## Need Help?

<Tip>
If you're unsure about a configuration option, load a template and see how it's structured. Templates are working examples you can learn from.
</Tip>

Click **"Give Feedback"** in the platform to ask questions or suggest documentation improvements.